# Importing necessary modules from the langchain package.
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# Creating a retriever from the existing vector store.
# The retriever is an essential component for fetching relevant documents or embeddings from the storage.
# The vector store instance is the one you created in the previous step.
retriever = vectorstore.as_retriever()

# Initializing a Retrieval-based Question Answering (QA) system.
# The system uses a 'map-reduce' type chain to handle and process the information.
# 'ChatOpenAI()' specifies the large language model to be used for generating responses.
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),  # Large Language Model for processing queries.
    chain_type="map_reduce",  # The processing chain type, orchestrating the retriever and the model.
    retriever=retriever  # The retriever instance that fetches relevant data for the QA system.
)

# Running the QA system with a specific query.
# This function triggers the LLM, starting with the retrieval of relevant information, followed by processing the query to generate a response.
response = qa.run("what is the retrieval qa chain?") 

# Outputting the response.
# The 'response' contains the answer generated by the QA system based on the retrieved documents and the internal logic of the 'ChatOpenAI' model.
print(response)  # It's advisable to handle this part more elegantly in real applications (e.g., formatting the response, handling errors, etc.).
